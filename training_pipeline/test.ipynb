{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from src.utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "session = sagemaker.Session()\n",
    "default_bucket = session.default_bucket()\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role_arn = iam.get_role(RoleName=f'101436505502-sagemaker-exec')['Role']['Arn']\n",
    "\n",
    "# load local csv\n",
    "df = pd.read_csv(\"data/mtsamples.csv\")\n",
    "df = df[df[\"transcription\"].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ENT - Otolaryngology\n",
      "PREOPERATIVE DIAGNOSES:,  OM, chronic, serous, simple or unspecified.  Adenoid hyperplasia.  Hypertrophy of tonsils.,POSTOPERATIVE DIAGNOSIS: , Same as preoperative diagnosis.,OPERATION: , Bilateral myringotomies with Armstrong grommet tubes, Adenoidectomy, and Tonsillectomy.,ANESTHESIA:,  General.,COMPLICATIONS:,  None.,ESTIMATED BLOOD LOSS: , Minimal.,DRAINS: , None.,CONSENT:,  The procedure, benefits, and risks were discussed in detail preoperatively.  The parentsagreed to proceed after all questions were answered.,TECHNIQUE: , The patient was brought to the operating room and placed in the supine position.  After general mask anesthesia was adequately obtained, the right external auditory canal was cleaned out under the microscope.  Serous fluid was aspirated from the middle ear space.  An Armstrong grommet tube was placed down through the incision and rotated into place.  The opposite ear was then cleaned out under the microscope.  Serous fluid was aspirated from the middle ear space.  An Armstrong grommet tube was placed down through the incision and rotated into place.  Cortisporin suspension was placed in both ear canals.,Then the patient was intubated.  A Crowe-Davis mouth gag was placed into the mouth and extended and hung on the Mayo stand.  The red rubber catheter was placed down through the nose and brought out through the mouth to retract the palate.  The adenoid fossa was visualized with the mirror.  The adenoids were removed using the microdebrider.  Two adenoid packs were placed.  The packs were removed one by one.  Using mirror and suction bovie, adequate hemostasis was achieved.,The tonsils were quite large and cryptic.  The tenaculum was placed on the superior pole of the right tonsil.  Cheesy material came out from the crypts.  The tonsils were retracted medially.  The bovie electrocautery was used to make an incision in the right anterior tonsillar pillar, and the plane was developed between the tonsil and the musculature.  The tonsil was completely dissected out of this plane, preserving both the anterior and posterior tonsillar pillars.  All bleeders were cauterized as they were encountered.  The tenaculum was then placed on the superior pole of the left tonsil.  Cheesy material came out from the crypts.  The tonsils were retracted medially.  The bovie electrocautery was used to make an incision in the left anterior tonsillar pillar, and the plane was developed between the tonsil and the musculature.  The tonsil was completely dissected out of this plane, preserving both the anterior and posterior tonsillar pillars.  All bleeders were cauterized as they were encountered.  Both tonsil beds were then re-cauterized, paying particular attention to the inferior and superior poles.,The stomach was evacuated with the nasogastric tube.  The patient was then awakened in the operating room, extubated and taken to the recovery room in satisfactory condition.\n"
     ]
    }
   ],
   "source": [
    "# get random sample\n",
    "idx = np.random.randint(len(df))\n",
    "input_txt = df.loc[idx].transcription\n",
    "category = df.loc[idx].medical_specialty\n",
    "print(category)\n",
    "print(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': [' Surgery']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"46-2023-04-11-08-57-35-184\"\n",
    "content_type = \"text/csv\" # \"application/json\" #\n",
    "accept = \"application/json\" #\"text/csv\"\n",
    "# payload = \"test\"#json.dumps({\"instances\":[\"test\"]})\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    # CustomAttributes=custom_attributes,\n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=input_txt,\n",
    ")\n",
    "\n",
    "json.loads(response[\"Body\"].read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model (for testing inference code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g4dn.xlarge\"\n",
    "pytorch_version = \"1.9.0\"\n",
    "transformers_version = \"4.11.0\"\n",
    "py_version = \"py38\"\n",
    "\n",
    "requirement_dependencies = [os.path.join(\n",
    "    # sys.path[0], \n",
    "    \"images\", \"inference\", \"requirements.txt\")]\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=\"text-classification-model-123\",\n",
    "    model_data=\"s3://sagemaker-eu-west-3-101436505502/model/pipelines-kam521dashky-train-model-JzSCBl76CF/output/model.tar.gz\",\n",
    "    sagemaker_session=session,\n",
    "    source_dir=\"src\",\n",
    "    entry_point=\"model.py\",\n",
    "    dependencies=['requirements.txt'],\n",
    "    role=role_arn,\n",
    "    transformers_version=transformers_version,\n",
    "    pytorch_version=pytorch_version,\n",
    "    py_version=py_version,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch inference (Transformer)\n",
    "Use latest approved model from model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_approved_model(model_package_group_name=\"training-pipelineModelGroup\"):\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    df = pd.DataFrame(sm_client.list_model_packages(\n",
    "        ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"])\n",
    "    \n",
    "    # models are allways sorted newst to oldest\n",
    "    model_package_arn = df.loc[df.ModelApprovalStatus == \"Approved\"].iloc[0].ModelPackageArn\n",
    "    \n",
    "    # return model\n",
    "    return sagemaker.ModelPackage(role=role_arn, model_package_arn=model_package_arn, sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: text-classification-model-123\n",
      "WARNING:sagemaker:Using already existing model: text-classification-model-123\n"
     ]
    }
   ],
   "source": [
    "input_data_path = f\"s3://{default_bucket}/data/test.csv\"\n",
    "output_data_path = f\"s3://{default_bucket}/data/out\"\n",
    "CONTENT_TYPE_CSV = 'text/csv'\n",
    "\n",
    "# load model\n",
    "model = get_latest_approved_model(model_package_group_name)\n",
    "\n",
    "# create transformer model\n",
    "transformer =model.transformer(\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.g4dn.xlarge', #\"ml.m5.large\",#\n",
    "    # strategy = 'SingleRecord',\n",
    "    # assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    accept = \"application/json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    data=input_data_path,\n",
    "    content_type=CONTENT_TYPE_CSV,\n",
    "    # split_type='Line',\n",
    ")\n",
    "# transformer.wait()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\") -> None:\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "    def tokenize(self, txt_input):\n",
    "        return self.tokenizer.encode(txt_input, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=40,\n",
    "    )\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y) -> None:\n",
    "        self.x = torch.tensor(x)\n",
    "        self.y = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 20714,  1024,  ...,     0,     0,     0],\n",
       "        [  101,  2627,  2966,  ...,  1010, 26572,   102],\n",
       "        [  101,  2381,  1997,  ...,  4645,  2545,   102],\n",
       "        ...,\n",
       "        [  101, 20714,  1024,  ...,  8048,  1999,   102],\n",
       "        [  101,  2708, 12087,  ..., 27179,  1024,   102],\n",
       "        [  101,  2381,  1024,  ..., 17758,  2135,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = MyTokenizer()\n",
    "texts = list(df.transcription.values)\n",
    "\n",
    "inputs = tok.tokenizer(texts, padding=\"max_length\", return_tensors='pt', truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/mx2blhp93k7blwkppkkv1w_r0000gn/T/ipykernel_9997/2213946952.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(x)\n",
      "/var/folders/b8/mx2blhp93k7blwkppkkv1w_r0000gn/T/ipykernel_9997/2213946952.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m (x, _) \u001b[39min\u001b[39;00m tqdm(\u001b[39mlist\u001b[39m(\u001b[39menumerate\u001b[39m(dataloader))[:\u001b[39m4\u001b[39m]):\n\u001b[0;32m----> 3\u001b[0m     outs \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      4\u001b[0m     output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mcpu(), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:759\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 759\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    760\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    761\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    762\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    763\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    764\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    765\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    766\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    768\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:563\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for (x, _) in tqdm(list(enumerate(dataloader))[:4]):\n",
    "    outs = model(x)\n",
    "    output += torch.argmax(outs.logits.cpu(), dim=1)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_nimbus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
