{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys, re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from src.utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "session = sagemaker.Session()\n",
    "default_bucket = session.default_bucket()\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role_arn = iam.get_role(RoleName=f'101436505502-sagemaker-exec')['Role']['Arn']\n",
    "\n",
    "# load local csv\n",
    "df = pd.read_csv(\"data/mtsamples.csv\")\n",
    "df = df[df[\"transcription\"].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Surgery': \n",
      " PREOPERATIVE DIAGNOSIS: , Pilonidal cyst with abscess formation.,POSTOPERATIVE DIAGNOSIS:,  Pilonidal cyst with abscess \n",
      "formation.,OPERATION: , Excision of infected pilonidal cyst.,PROCEDURE: , After obtaining informed consent, the patient \n",
      "underwent a spinal anesthetic and was placed in the prone position in the operating room.  A time-out process was follow\n",
      "ed.  Antibiotics were given and then the patient was prepped and draped in the usual fashion.  It appeared to me that th\n",
      "e abscess had dr ... \n",
      "\n",
      "' Cardiovascular / Pulmonary': \n",
      " HISTORY OF PRESENT ILLNESS: , The patient is a 71-year-old woman with history of coronary artery disease for which she h\n",
      "as had coronary artery bypass grafting x2 and percutaneous coronary intervention with stenting x1.  She also has a signi\n",
      "ficant history of chronic renal insufficiency and severe COPD.  The patient and her husband live in ABC but they have fa\n",
      "mily in XYZ.  She came to our office today as she is in the area visiting her family.  She complains of having shortness\n",
      " of breath for t ... \n",
      "\n",
      "' Podiatry': \n",
      " S:,  The patient presents to Podiatry Clinic today for initial examination, evaluation, and treatment of her nails.,PRIM\n",
      "ARY MEDICAL HISTORY:,  Adenocarcinoma, delirium, recent dehydration, anemia, history of hypertension, and hyperlipidemia\n",
      ".,MEDICATIONS: , Refer to chart.,ALLERGIES: , PENICILLIN AND ASPIRIN.,O: , The patient  presents in wheelchair, verbal a\n",
      "nd alert.  Vascular:  She has absent pedal pulses bilaterally.  Trophic changes include absent hair growth and mycotic n\n",
      "ails.  Skin text ... \n",
      "\n",
      "' Surgery': \n",
      " PREOPERATIVE DIAGNOSES:,1.  Hoarseness.,2.  Bilateral true vocal cord lesions.,3.  Leukoplakia.,POSTOPERATIVE DIAGNOSES:\n",
      ",1.  Hoarseness.,2.  Bilateral true vocal cord lesions.,3.  Leukoplakia.,PROCEDURE PERFORMED:  ,Microscopic suspension d\n",
      "irect laryngoscopy with biopsy of left true vocal cord stripping.,ANESTHESIA:,  General endotracheal.,ESTIMATED BLOOD LO\n",
      "SS:,  Minimal.,COMPLICATIONS: , None.,INDICATIONS FOR PROCEDURE:  The patient is a 33-year-old Caucasian male with a his\n",
      "tory of chronic  ... \n",
      "\n",
      "' Consult - History and Phy.': \n",
      " CHIEF COMPLAINT:,  A 2-month-old female with 1-week history of congestion and fever x2 days.,HISTORY OF PRESENT ILLNESS:\n",
      ",  The patient is a previously healthy 2-month-old female, who has had a cough and congestion for the past week.  The mo\n",
      "ther has also reported irregular breathing, which she describes as being rapid breathing associated with retractions.  T\n",
      "he mother states that the cough is at times paroxysmal and associated with posttussive emesis.  The patient has had shor\n",
      "t respiratory pa ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get random sample\n",
    "n_samples = 5\n",
    "idx = np.random.randint(len(df), size=n_samples)\n",
    "inputs = df.loc[idx].transcription.tolist()\n",
    "targets = df.loc[idx].medical_specialty.tolist()\n",
    "\n",
    "for t, i in zip(targets, inputs):\n",
    "    text_block = re.sub(\"(.{120})\", \"\\\\1\\n\", i, 0, re.DOTALL)\n",
    "    print(f\"'{t}': \\n {text_block[:500]} ... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consult - History and Phy.</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consult - History and Phy.</td>\n",
       "      <td>Podiatry</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consult - History and Phy.</td>\n",
       "      <td>Consult - History and Phy.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pred                       target  correct\n",
       "0                      Surgery                      Surgery     True\n",
       "1   Consult - History and Phy.   Cardiovascular / Pulmonary    False\n",
       "2   Consult - History and Phy.                     Podiatry    False\n",
       "3                      Surgery                      Surgery     True\n",
       "4   Consult - History and Phy.   Consult - History and Phy.     True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"1-2023-04-18-08-02-20-615\"\n",
    "CONTENT_TYPE_JSON = \"application/json\"\n",
    "payload = json.dumps({\"instances\":inputs})\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=CONTENT_TYPE_JSON,\n",
    "    Accept=CONTENT_TYPE_JSON,\n",
    "    Body=payload,\n",
    ")\n",
    "\n",
    "prediction = json.loads(response[\"Body\"].read())[\"prediction\"]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results[\"pred\"] = prediction\n",
    "results[\"target\"] = targets\n",
    "results[\"correct\"] = results.apply(lambda x: x.pred == x.target, axis=1)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get new model (only for testing model inference code)\n",
    "Use model data from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g4dn.xlarge\"\n",
    "pytorch_version = \"1.9.0\"\n",
    "transformers_version = \"4.11.0\"\n",
    "py_version = \"py38\"\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=\"text-classification-model-123\",\n",
    "    model_data=\"s3://sagemaker-eu-west-3-101436505502/model/pipelines-kam521dashky-train-model-JzSCBl76CF/output/model.tar.gz\",\n",
    "    sagemaker_session=session,\n",
    "    source_dir=\"src\",\n",
    "    entry_point=\"model.py\",\n",
    "    dependencies=['requirements.txt'],\n",
    "    role=role_arn,\n",
    "    transformers_version=transformers_version,\n",
    "    pytorch_version=pytorch_version,\n",
    "    py_version=py_version,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch inference (Transformer)\n",
    "Use latest approved model from model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ModelPackageGroupName': 'training-pipelineModelGroup', 'ModelPackageVersion': 1, 'ModelPackageArn': 'arn:aws:sagemaker:eu-west-3:101436505502:model-package/training-pipelinemodelgroup/1', 'CreationTime': datetime.datetime(2023, 4, 14, 15, 50, 32, 258000, tzinfo=tzlocal()), 'ModelPackageStatus': 'Completed', 'ModelApprovalStatus': 'Approved'}]\n",
      "The latest approved model-arn is: arn:aws:sagemaker:eu-west-3:101436505502:model-package/training-pipelinemodelgroup/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:eu-west-3:101436505502:model-package/training-pipelinemodelgroup/1'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_latest_approved_model(model_package_group_name):\n",
    "    \"\"\"Retrieves the latest approved model from a given SageMaker model package group.\"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    df = pd.DataFrame(sm_client.list_model_packages(\n",
    "        ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"])\n",
    "    model_package_ arns = sm_client.list_model_packages(\n",
    "        ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"]\n",
    "    asdasd\n",
    "    try:\n",
    "        model_package_arn = df.loc[df.ModelApprovalStatus ==\n",
    "                                   \"Approved\"].iloc[0].ModelPackageArn\n",
    "        print(f\"The latest approved model-arn is: {model_package_arn}\")\n",
    "        return model_package_arn\n",
    "\n",
    "    except IndexError:\n",
    "        raise SystemExit(\n",
    "            f\"There is no approved model in the model-group '{model_package_group_name}'\")\n",
    "        \n",
    "        \n",
    "get_latest_approved_model(\"training-pipelineModelGroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_approved_model(model_package_group_name):\n",
    "    \"\"\"Retrieves the latest approved model from a given SageMaker model package group.\"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    df = pd.DataFrame(sm_client.list_model_packages(\n",
    "        ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"])\n",
    "    model_package_arn = df.loc[df.ModelApprovalStatus ==\n",
    "                               \"Approved\"].iloc[0].ModelPackageArn\n",
    "    print(f\"The latest approved model-arn is: {model_package_arn}\")\n",
    "    return sagemaker.ModelPackage(role=role_arn, model_package_arn=model_package_arn, sagemaker_session=session)\n",
    "\n",
    "\n",
    "def run_batch_inference(input_data_path, output_data_path):\n",
    "    CONTENT_TYPE_CSV = 'text/csv'\n",
    "    CONTENT_TYPE_JSON = \"application/json\"\n",
    "\n",
    "    model = get_latest_approved_model(\"training-pipelineModelGroup\")\n",
    "    transformer = model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.g4dn.xlarge',  # \"ml.m5.large\"\n",
    "        output_path=output_data_path,\n",
    "        accept=CONTENT_TYPE_CSV,\n",
    "        # strategy = 'SingleRecord',\n",
    "        # assemble_with = 'Line',\n",
    "    )\n",
    "\n",
    "    transformer.transform(\n",
    "        data=input_data_path,\n",
    "        content_type=CONTENT_TYPE_CSV,\n",
    "        # split_type='Line',\n",
    "    )\n",
    "    transformer.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest approved model-arn is: arn:aws:sagemaker:eu-west-3:101436505502:model-package/training-pipelinemodelgroup/52\n"
     ]
    }
   ],
   "source": [
    "input_data_path = f\"s3://{default_bucket}/data/test.csv\"\n",
    "output_data_path = f\"s3://{default_bucket}/data/out\"\n",
    "\n",
    "run_batch_inference(input_data_path, output_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\") -> None:\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "    def tokenize(self, txt_input):\n",
    "        return self.tokenizer.encode(txt_input, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=40,\n",
    "    )\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y) -> None:\n",
    "        self.x = torch.tensor(x)\n",
    "        self.y = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 20714,  1024,  ...,     0,     0,     0],\n",
       "        [  101,  2627,  2966,  ...,  1010, 26572,   102],\n",
       "        [  101,  2381,  1997,  ...,  4645,  2545,   102],\n",
       "        ...,\n",
       "        [  101, 20714,  1024,  ...,  8048,  1999,   102],\n",
       "        [  101,  2708, 12087,  ..., 27179,  1024,   102],\n",
       "        [  101,  2381,  1024,  ..., 17758,  2135,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = MyTokenizer()\n",
    "texts = list(df.transcription.values)\n",
    "\n",
    "inputs = tok.tokenizer(texts, padding=\"max_length\", return_tensors='pt', truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/mx2blhp93k7blwkppkkv1w_r0000gn/T/ipykernel_9997/2213946952.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(x)\n",
      "/var/folders/b8/mx2blhp93k7blwkppkkv1w_r0000gn/T/ipykernel_9997/2213946952.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m (x, _) \u001b[39min\u001b[39;00m tqdm(\u001b[39mlist\u001b[39m(\u001b[39menumerate\u001b[39m(dataloader))[:\u001b[39m4\u001b[39m]):\n\u001b[0;32m----> 3\u001b[0m     outs \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      4\u001b[0m     output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mcpu(), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:759\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 759\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    760\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    761\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    762\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    763\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    764\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    765\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    766\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    768\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:563\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for (x, _) in tqdm(list(enumerate(dataloader))[:4]):\n",
    "    outs = model(x)\n",
    "    output += torch.argmax(outs.logits.cpu(), dim=1)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage Sagemaker resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.experiments.experiment import _Experiment\n",
    "\n",
    "experiment_name = \"q9lp5qy5wd0f-tune-mo-8zsgutfsto\"\n",
    "exp = _Experiment.load(experiment_name=experiment_name, sagemaker_session=sagemaker.Session())\n",
    "exp._delete_all(action=\"--force\")\n",
    "\n",
    "# sm = boto3.Session().client('sagemaker')\n",
    "# sm.delete_experiment(ExperimentName=experiment_name)\n",
    "\n",
    "def empty_and_delete_model_package(sagemaker_client, mpg_name):\n",
    "    mpg = sagemaker_client.list_model_packages(\n",
    "        ModelPackageGroupName=mpg_name,\n",
    "    )\n",
    "    \n",
    "    # Delete model packages if Group not empty\n",
    "    model_packages = mpg.get('ModelPackageSummaryList')\n",
    "    if model_packages:\n",
    "        for mp in model_packages:\n",
    "            sagemaker_client.delete_model_package(\n",
    "                ModelPackageName=mp['ModelPackageArn']\n",
    "            )\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Delete model package group\n",
    "    sagemaker_client.delete_model_package_group(\n",
    "        ModelPackageGroupName=mpg_name\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "import time\n",
    "\n",
    "client = boto3.client(service_name='sagemaker')\n",
    "mpg_name='training-pipelineModelGroup'\n",
    "\n",
    "empty_and_delete_model_package(client, mpg_name)\n",
    "\n",
    "\n",
    "import boto3\n",
    "\n",
    "search_params={\n",
    "   # \"MaxResults\": 10,\n",
    "   \"Resource\": \"ExperimentTrial\",\n",
    "#    \"SearchExpression\": { \n",
    "#       \"Filters\": [{ \n",
    "#             \"Name\": \"Tags.Project\",\n",
    "#             \"Operator\": \"Equals\",\n",
    "#             \"Value\": \"Project_Binary_Classifier\"\n",
    "#          }]},\n",
    "#   \"SortBy\": \"Trail.CreationTime\",\n",
    "  \"SortOrder\": \"Ascending\"\n",
    "}\n",
    "\n",
    "smclient = boto3.client(service_name='sagemaker')\n",
    "results = smclient.search(**search_params)\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_nimbus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
