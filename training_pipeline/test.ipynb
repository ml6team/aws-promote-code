{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from src.utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "session = sagemaker.Session()\n",
    "default_bucket = session.default_bucket()\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role_arn = iam.get_role(RoleName=f'101436505502-sagemaker-exec')['Role']['Arn']\n",
    "\n",
    "# load local csv\n",
    "df = pd.read_csv(\"data/mtsamples.csv\")\n",
    "df = df[df[\"transcription\"].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Radiology\n",
      "EXAM:,MRI LEFT SHOULDER,CLINICAL:,This is a 69-year-old male with pain in the shoulder. Evaluate for rotator cuff tear.,FINDINGS:,Examination was performed on 9/1/05.,There is marked supraspinatus tendinosis and extensive tearing of the substance of the tendon and articular surface, extending into the myotendinous junction as well. There is still a small rim of tendon along the bursal surface, although there may be a small tear at the level of the rotator interval. There is no retracted tendon or muscular atrophy (series #6 images #6-17).,Normal infraspinatus tendon.,There is subscapularis tendinosis with fraying and partial tearing of the superior most fibers extending to the level of the rotator interval (series #9 images #8-13; series #3 images #8-14). There is no complete tear, gap or fiber retraction and there is no muscular atrophy.,There is tendinosis and superficial tearing of the long biceps tendon within the bicipital groove, and there is high grade (near complete) partial tearing of the intracapsular portion of the tendon. The biceps anchor is intact. There are degenerative changes in the greater tuberosity of the humerus but there is no fracture or subluxation.,There is degeneration of the superior labrum and there is a small nondisplaced tear in the posterior superior labrum at the one to two o’clock position (series #6 images #12-14; series #3 images #8-10; series #9 images #5-8). There is a small sublabral foramen at the eleven o’clock position (series #9 image #6). There is no osseous Bankart lesion.,Normal superior, middle and inferior glenohumeral ligaments.,There is hypertrophic osteoarthropathy of the acromioclavicular joint with narrowing of the subacromial space and flattening of the superior surface of the supraspinatus musculotendinous junction, which in the appropriate clinical setting is an MRI manifestation of an impinging lesion (series #8 images #3-12).,Normal coracoacromial, coracohumeral and coracoclavicular ligaments. There is minimal fluid within the glenohumeral joint. There is no atrophy of the deltoid muscle.,IMPRESSION:,  There is extensive supraspinatus tendinosis and partial tearing as described. There is no retracted tendon or muscular atrophy, but there may be a small tear along the anterior edge of the tendon at the level of the rotator interval, and this associated partial tearing of the superior most fibers of the subscapularis tendon.  There is also a high-grade partial tear of the long biceps tendon as it courses under the transverse humeral ligament. There is no evidence of a complete tear or retracted tendon. Small nondisplaced posterior superior labral tear. Outlet narrowing from the acromioclavicular joint, which in the appropriate clinical setting is an MRI manifestation of an impinging lesion.\n"
     ]
    }
   ],
   "source": [
    "# get random sample\n",
    "idx = np.random.randint(len(df))\n",
    "input_txt = df.loc[idx].transcription\n",
    "category = df.loc[idx].medical_specialty\n",
    "\n",
    "print(category)\n",
    "print(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': [' Orthopedic']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"53-2023-04-12-13-25-22-053\"\n",
    "content_type = \"application/json\" #\n",
    "accept = \"application/json\" #\"text/csv\"\n",
    "payload = json.dumps({\"instances\":[input_txt]})\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=payload,\n",
    ")\n",
    "\n",
    "json.loads(response[\"Body\"].read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model (for testing model inference code)\n",
    "Use model data from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_instance_type = \"ml.g4dn.xlarge\"\n",
    "# pytorch_version = \"1.9.0\"\n",
    "# transformers_version = \"4.11.0\"\n",
    "# py_version = \"py38\"\n",
    "\n",
    "# requirement_dependencies = [os.path.join(\n",
    "#     # sys.path[0], \n",
    "#     \"images\", \"inference\", \"requirements.txt\")]\n",
    "\n",
    "# model = HuggingFaceModel(\n",
    "#     name=\"text-classification-model-123\",\n",
    "#     model_data=\"s3://sagemaker-eu-west-3-101436505502/model/pipelines-kam521dashky-train-model-JzSCBl76CF/output/model.tar.gz\",\n",
    "#     sagemaker_session=session,\n",
    "#     source_dir=\"src\",\n",
    "#     entry_point=\"model.py\",\n",
    "#     dependencies=['requirements.txt'],\n",
    "#     role=role_arn,\n",
    "#     transformers_version=transformers_version,\n",
    "#     pytorch_version=pytorch_version,\n",
    "#     py_version=py_version,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch inference (Transformer)\n",
    "Use latest approved model from model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_approved_model(model_package_group_name):\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    df = pd.DataFrame(sm_client.list_model_packages(\n",
    "        ModelPackageGroupName=model_package_group_name)[\"ModelPackageSummaryList\"])\n",
    "    model_package_arn = df.loc[df.ModelApprovalStatus == \"Approved\"].iloc[0].ModelPackageArn\n",
    "    print(f\"The latest approved model-arn is: {model_package_arn}\")\n",
    "    return sagemaker.ModelPackage(role=role_arn, model_package_arn=model_package_arn, sagemaker_session=session)\n",
    "\n",
    "\n",
    "def run_batch_inference(input_data_path, output_data_path):\n",
    "    CONTENT_TYPE_CSV = 'text/csv'\n",
    "    CONTENT_TYPE_JSON = \"application/json\"\n",
    "\n",
    "    model = get_latest_approved_model(\"training-pipelineModelGroup\")\n",
    "    transformer =model.transformer(\n",
    "        instance_count = 1,\n",
    "        instance_type = 'ml.g4dn.xlarge', #\"ml.m5.large\"\n",
    "        output_path = output_data_path,\n",
    "        accept = CONTENT_TYPE_CSV,\n",
    "        # strategy = 'SingleRecord',\n",
    "        # assemble_with = 'Line',\n",
    "    )\n",
    "    \n",
    "    transformer.transform(\n",
    "        data=input_data_path,\n",
    "        content_type=CONTENT_TYPE_CSV,\n",
    "        # split_type='Line',\n",
    "    )\n",
    "    transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:eu-west-3:101436505502:model-package/training-pipelinemodelgroup/53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: 53-2023-04-12-12-35-31-941\n"
     ]
    }
   ],
   "source": [
    "input_data_path = f\"s3://{default_bucket}/data/test.csv\"\n",
    "output_data_path = f\"s3://{default_bucket}/data/out\"\n",
    "\n",
    "run_batch_inference(input_data_path, output_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\") -> None:\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "    def tokenize(self, txt_input):\n",
    "        return self.tokenizer.encode(txt_input, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=40,\n",
    "    )\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y) -> None:\n",
    "        self.x = torch.tensor(x)\n",
    "        self.y = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 20714,  1024,  ...,     0,     0,     0],\n",
       "        [  101,  2627,  2966,  ...,  1010, 26572,   102],\n",
       "        [  101,  2381,  1997,  ...,  4645,  2545,   102],\n",
       "        ...,\n",
       "        [  101, 20714,  1024,  ...,  8048,  1999,   102],\n",
       "        [  101,  2708, 12087,  ..., 27179,  1024,   102],\n",
       "        [  101,  2381,  1024,  ..., 17758,  2135,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = MyTokenizer()\n",
    "texts = list(df.transcription.values)\n",
    "\n",
    "inputs = tok.tokenizer(texts, padding=\"max_length\", return_tensors='pt', truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/mx2blhp93k7blwkppkkv1w_r0000gn/T/ipykernel_9997/2213946952.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x = torch.tensor(x)\n",
      "/var/folders/b8/mx2blhp93k7blwkppkkv1w_r0000gn/T/ipykernel_9997/2213946952.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(inputs.input_ids, inputs.attention_mask)\n",
    "\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m (x, _) \u001b[39min\u001b[39;00m tqdm(\u001b[39mlist\u001b[39m(\u001b[39menumerate\u001b[39m(dataloader))[:\u001b[39m4\u001b[39m]):\n\u001b[0;32m----> 3\u001b[0m     outs \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      4\u001b[0m     output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mcpu(), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:759\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 759\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    760\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    761\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    762\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    763\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    764\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    765\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    766\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    768\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:563\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for (x, _) in tqdm(list(enumerate(dataloader))[:4]):\n",
    "    outs = model(x)\n",
    "    output += torch.argmax(outs.logits.cpu(), dim=1)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_nimbus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
