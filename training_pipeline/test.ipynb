{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboto3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data/mtsamples.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cardiovascular / Pulmonary\n",
      "SUBJECTIVE: , Review of the medical record shows that the patient is a 97-year-old female patient who has been admitted and has been treated for community acquired pneumonia along with COPD exacerbation.  The patient does have a longstanding history of COPD.  However, she does not use oxygen at her independent assisted living home.  Yesterday, she had made improvement since being here at the hospital.  She needed oxygen.  She was tested for home O2 and qualified for it yesterday also.  Her lungs were very tight.  She did have wheezes bilaterally and rhonchi on the right side mostly.  She appeared to be a bit weak and although she was requesting to be discharged home, she did not appear to be fit for it.,Overnight, the patient needed to use the rest room.  She stated that she needed to urinate.  She awoke, decided not to call for assistance.  She stated that she did have her nurse call light button next to her and she was unable to gain access to her walker.  She attempted to walk to the rest room on her own.  She sustained a fall.  She stated that she just felt weak.  She bumped her knee and her elbow.  She had femur x-rays, knee x-rays also.  There was possibility of subchondral fracture and some swelling of her suprapatellar bursa on the right side.  This morning, she denied any headache, back pain or neck pain.  She complained mostly of right anterior knee pain for which she had some bruising and swelling.,OBJECTIVE:,VITAL SIGNS:  The patient's max temperature over the past 24 hours was 36.5; her blood pressure is 148/77, her pulse is 87 to 106.  She is 95% on 2 L via nasal cannula.,HEART:  Regular rate and rhythm without murmur, gallop or rub.,LUNGS:  Reveal no expiratory wheezing throughout.  She does have some rhonchi on the right mid base.  She did have a productive cough this morning and she is coughing green purulent sputum finally.,ABDOMEN:  Soft and nontender.  Her bowel sounds x4 are normoactive.,NEUROLOGIC:  She is alert and oriented x3.  Her pupils are equal and reactive.  She has got a good head and facial muscle strength.  Her tongue is midline.  She has got clear speech.  Her extraocular motions are intact.  Her spine is nontender on palpation from neck to lumbar spine.  She has good range of motion with regard to her shoulders, elbows, wrists and fingers.  Her grip strengths are equal bilaterally.  Both elbows are strong from extension to flexion.  Her hip flexors and extenders are also strong and equal bilaterally.  Extension and flexion of the knee bilaterally and ankles also are strong.  Palpation of her right knee reveals no crepitus.  She does have suprapatellar inflammation with some ecchymosis and swelling.  She has got good joint range of motion however.,SKIN:  She did have a skin tear involving her right forearm lateral, which is approximately 2 to 2.5 inches in length and is at this time currently Steri-Stripped and wrapped with Coban and is not actively bleeding.,ASSESSMENT:,1.  Acute on chronic COPD exacerbation.,2.  Community acquired pneumonia both resolving.  However, she may need home O2 for a short period of time.,3.  Generalized weakness and deconditioning secondary to the above.  Also sustained a fall secondary to instability and not using her walker or calling for assistance.  The patient stated that she knew better and she should have called for assistance and she had been told repeatedly from her family members and staff to call for assistance if she needed to get out of bed.,PLAN:,1.  I will have PT and OT evaluate the patient and give recommendation to safety and appliance use at home i.e. walker.  Myself and one of her daughter's spoke today about the fact that she generally lives independently at the Brooke and she may need assisted living along with physical therapy and oxygen for a period of time rather than going back to independent living.,2.  We will obtain an orthopedic consult secondary to her fall to evaluate her x-rays and function.\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(df))\n",
    "input_txt = df.loc[idx].transcription\n",
    "category = df.loc[idx].medical_specialty\n",
    "print(category)\n",
    "print(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"28-2023-04-04-06-38-09-524\"\n",
    "content_type = \"text/csv\" # \"application/json\" #\n",
    "accept = \"application/json\" #\"text/csv\"\n",
    "# payload = \"test\"#json.dumps({\"instances\":[\"test\"]})\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    # CustomAttributes=custom_attributes,\n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=input_txt,\n",
    ")\n",
    "\n",
    "json.loads(response[\"Body\"].read())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch inference (Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eu-west-3'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "default_bucket = session.default_bucket()\n",
    "\n",
    "model_name = \"22-2023-04-03-13-43-05-767\"\n",
    "input_data_path = f\"s3://{default_bucket}/data/test.csv\"\n",
    "output_data_path = f\"s3://{default_bucket}/data/out\"\n",
    "CONTENT_TYPE_CSV = 'text/csv'\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role_arn = iam.get_role(RoleName=f'101436505502-sagemaker-exec')['Role']['Arn']\n",
    "model_package_arn = \"arn:aws:sagemaker:eu-west-3:101436505502:\" \\\n",
    "                        f\"model-package/training-pipelineModelGroup/22\"\n",
    "\n",
    "\n",
    "# c = boto3.client('sagemaker')\n",
    "# c.list_model_packages(ModelPackageGroupName=\"training-pipelineModelGroup\")\n",
    "\n",
    "\n",
    "transform_job = sagemaker.transformer.Transformer(\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.g4dn.xlarge', # ml.g4dn.xlarge\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='inference-pipelines-batch',\n",
    "    sagemaker_session=session,\n",
    "    accept = CONTENT_TYPE_CSV)\n",
    "\n",
    "transform_job.transform(data = input_data_path, \n",
    "                        content_type = CONTENT_TYPE_CSV, \n",
    "                        split_type = 'Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: 22-2023-04-03-13-51-02-471\n"
     ]
    }
   ],
   "source": [
    "model = sagemaker.ModelPackage(role=role_arn, model_package_arn=model_package_arn,\n",
    "                         sagemaker_session=session)\n",
    "\n",
    "transformer =model.transformer(\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.g4dn.xlarge', # ml.g4dn.xlarge\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    accept = CONTENT_TYPE_CSV,\n",
    "    # region=\"eu-west-3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(data = input_data_path, \n",
    "                        content_type = CONTENT_TYPE_CSV, \n",
    "                        split_type = 'Line')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\") -> None:\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "    def tokenize(self, txt_input):\n",
    "        return self.tokenizer.encode(txt_input, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=40,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (886) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m y_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mcpu(), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m y_pred\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:759\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 759\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    760\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    761\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    762\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    763\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    764\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    765\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    766\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    768\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:578\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    575\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    580\u001b[0m     x\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    581\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    586\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1533\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[39m# this function, and just call forward.  It's slow for dynamo to guard on the state\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m# of all these hook dicts individually, so instead it can guard on 2 bools and we just\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# have to promise to keep them up to date when hooks are added or removed via official means.\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_hooks \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_global_hooks:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1534\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:130\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    127\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings(input_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m embeddings \u001b[39m=\u001b[39m word_embeddings \u001b[39m+\u001b[39;49m position_embeddings  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (886) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "tok = MyTokenizer()\n",
    "# input = tok.tokenizer(input_txt, padding=\"max_length\", return_tensors='pt', truncation=True)\n",
    "input = tok.tokenizer(input_txt, return_tensors='pt')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "input.to(device)\n",
    "outputs = model(**input)\n",
    "y_pred = torch.argmax(outputs.logits.cpu(), dim=1)\n",
    "y_pred\n",
    "# [config.MEDICAL_CATEGORIES[i.item()] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_nimbus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
